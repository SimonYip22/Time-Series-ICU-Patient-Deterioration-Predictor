# Neural Network (TCN) Time-Series ICU Deterioration Predictor (PyTorch & LightGBM) ğŸ“ˆâ³ 

Early warning system predicting ICU patient deterioration on MIMIC-IV Clinical Demo v2.2 dataset (100 patients), framework comparing LightGBM vs Temporal Convolutional Network across 3 targets: peak deterioration (max_risk), typical risk (median_risk), and proportion of admission in high-risk states (pct_time_high).

Temporal and aggregated feature engineering with clinical-validity-aware missing data handling, using custom NEWS2-derived ground-truth values (GCS/LOC mapping, supplemental Oâ‚‚ and COâ‚‚ retainer custom logic).

TCN trained on 171 temporal-features x 96 timestamps, 24hr rolling windows across 8 vital parameters, with 3-layer TemporalBlock stack, kernel=3, dropout=0.2, head_hidden=64, batch=32, 50 epochs, early stopping at epoch 10;

LightGBM trained on 40 aggregated patient-features with 5-fold stratified CV, with hyperparameter tuning.

TCN greater sensivity on max_risk (AUC +9.3%, AP +1.25%), LightGBM greater reliability and calibration on median_risk (AUC +17%, Brier â†“68%, ECE â†“63%) and more precise pct_time_high (RMSE â†“32%, RÂ² +44%, residual SD â†“42%).

evaluating interpretability (SHAP) versus saliency-based explanations for clinical adoption.

Deployed, reproducible auditable pipeline with deployment-lite, and full documentation for clinical validation.

TCN_refined captures short-term acute events with rapid early detection, whereas LightGBM provides robust, calibrated estimates of sustained deterioration exposure; supports ICU triage, continuous monitoring, and escalation decisions with quantified, actionable confidence.
TCN excels at short-term acute events with rapid detection while LightGBM provides reliable long-term estimates of sustained deterioration exposure, suggesting ensemble approach for production deployment.

Portfolio-ready, deployed, and clinically-informed.

**Tech stack**: python, pandas, NumpPy, LightGBM, PyTorch

**Pipeline**
```text
Raw ICU Vitals (long format, MIMIC-style)
   â””â”€> compute_news2.py
         â”œâ”€ Input: raw vitals CSV
         â”œâ”€ Action: compute NEWS2 scores per timestamp
         â””â”€ Output: news2_scores.csv (wide format with vitals, NEWS2 score, escalation labels), news2_patient_summary.csv (patient-level summary)

news2_scores.csv
   â””â”€> make_timestamp_features.py
         â”œâ”€ Action:
         â”‚   â”œâ”€ Aggregate per patient
         â”‚   â”œâ”€ Add missingness flags
         â”‚   â”œâ”€ Apply LOCF per vital
         â”‚   â”œâ”€ Compute carried-forward flags
         â”‚   â”œâ”€ Compute rolling window stats (1h/4h/24h)
         â”‚   â”œâ”€ Compute time-since-last-observation
         â”‚   â””â”€ Encode risk/escalation as ordinal numeric
         â””â”€ Output: news2_features_timestamp.csv
               (ML-ready timestamp-level features)

news2_scores.csv
   â””â”€> make_patient_features.py
         â”œâ”€ Action:
         â”‚   â”œâ”€ Aggregate per patient
         â”‚   â”œâ”€ Compute median, mean, min, max per vital
         â”‚   â””â”€ Include % missingness per vital
         â””â”€ Output: news2_features_patient.csv
               (ML-ready patient-level summary features)
```

# Timestamp features rationale
- We compute rolling window features over 1h, 4h, and 24h intervals. 
   - Mean, min, max capture the magnitude and variability of vitals. 
   - Slope gives the trend â€” whether the vital is rising or falling and how fast. 
   - AUC measures cumulative exposure, i.e., how much and for how long a patient has experienced abnormal values. 
- These features provide temporal context for the ML model, so it doesnâ€™t just see isolated values but also their trajectory over time.


# LightGBM vs Neural Network (TCN) Pipeline
```text
ML Model (LightGBM)
   â”œâ”€ Input: news2_features_patient.csv 
   â”‚     â”œâ”€ Median, mean, min, max per vital
   â”‚     â”œâ”€ Impute missing values
   â”‚     â”œâ”€ % missing per vital
   â”‚     â””â”€ Risk summary stats (max, median, % time at high risk)
   â”œâ”€ Action:
   â”‚     â”œâ”€ Train predictive model for deterioration / escalation
   â”‚     â”œâ”€ Use timestamp trends + missingness flags
   â”‚     â””â”€ Evaluate performance (AUROC, precision-recall, etc.)
   â””â”€ Output: predictions, feature importances, evaluation metrics

ML Model (Neural Network, TCN)
   â”œâ”€ Input: news2_features_timestamp.csv
   â”‚     â”œâ”€ Timestamp-level vitals & rolling features (mean, min, max, std, slopes, AUC)
   â”‚     â”œâ”€ Missingness flags
   â”‚     â”œâ”€ Carried-forward flags  
   â”‚     â””â”€ Time since last observation
   â”œâ”€ Action:
   â”‚     â”œâ”€ Train predictive model for deterioration / escalation
   â”‚     â”œâ”€ Learn temporal patterns, trends, and interactions
   â”‚     â”œâ”€ Can handle sequences of variable length per patient
   â”‚     â””â”€ Evaluate performance (AUROC, precision-recall, calibration)
   â””â”€ Output: 
         â”œâ”€ Predictions per timestamp or per patient
         â”œâ”€ Learned feature embeddings / attention weights (if applicable)
         â””â”€ Evaluation metrics
```

# LightGBM vs Neural Network (TCN) Pipeline Visualisation
```text
  Raw EHR Data (vitals, observations, lab results)
         â”‚
         â–¼
Timestamp Feature Engineering (news2_scores.csv)
 - Rolling statistics (mean, min, max, std)
 - Slopes, AUC, time since last observation
 - Imputation & missingness flags
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TCN Neural Network Model (v2)
         â”‚              - Input: full time-series per patient
         â”‚              - Can learn temporal patterns, trends, dynamics
         â”‚
         â–¼
Patient-Level Feature Aggregation (make_patient_features.py â†’ news2_features_patient.csv)
 - Median, mean, min, max per vital
 - % missing per vital
 - Risk summary stats (max, median, % time at high risk)
 - Ordinal encoding for risk/escalation
         â”‚
         â–¼
LightGBM Model (v1)
 - Input: one row per patient (fixed-length vector)
 - Uses aggregated statistics only
 - Cannot handle sequences or variable-length time series
```

 # Model Comparison: LightGBM vs Neural Network (V1 & V2)

| Aspect | LightGBM (V1) | Temporal Convolutional Network (TCN) (V2) |
|--------|-------------------|-------------------|
| **ML Model Name / Type** | LightGBM (Gradient Boosted Decision Trees) | Temporal Convolutional Network (TCN)(Neural network) |
| **V1 / V2** | V1: uses patient-level features, baseline interpretable patient summary (classic tabular ML) | V2: uses timestamp-level features, advanced sequence modeling (modern deep learning) |
| **Input Datasets** | `news2_features_patient.csv` (patient-level summaries) | `news2_features_timestamp.csv` (time series of vitals, missingness flags) |
| **Optional Inputs** | Timestamp features could be added later for hybrid model | Patient-level summary features from `news2_features_patient.csv` can be appended but not mandatory |
| **Reason for this input choice** | LightGBM is a tree-based model: handles static features and aggregates well; does not naturally model temporal sequences | Neural networks (LSTM/TCN) can model temporal trends, sequences, and interactions over time; need full timestamp features to exploit sequential information |
| **Why two different models** | LightGBM: fast, interpretable (feature importance), strong baseline. | Neural network: captures temporal dynamics, can potentially improve predictive performance on time-series deterioration<br>Complements LightGBM; addresses potential limitations of static patient summaries by using sequential information in timestamp features |
| **Strengths** | - Handles missing values gracefully.<br>- Fast training and inference.<br>- Provides feature importances.<br>- Works well with tabular summary features. | - Models temporal trends and interactions.<br>- Can capture subtle patterns in sequences of vitals.<br>- Potentially better performance on real-time deterioration prediction. |
| **Weaknesses / Limitations** | - Ignores sequence and timing of events.<br>- May lose some granularity of patient trajectory.<br>- Cannot capture interactions over time. | - Requires more computation and tuning.<br>- Harder to interpret.<br>- Sensitive to missing data; requires careful imputation or masking. |
| **Output** | Predictions per patient, feature importances, evaluation metrics (AUROC, PR-AUC, etc.) | Predictions per timestamp or per patient trajectory, evaluation metrics (AUROC, PR-AUC, potentially time-dependent metrics) |
| **Use case / Deployment** | Baseline model; interpretable; fast deployment; can be used for early warning systems using summary features | Advanced model for final deployment or v2 experimentation; may be integrated in real-time monitoring dashboards for continuous deterioration prediction |


Portfolio narrative framing (objective and honest)

Hereâ€™s how you can present this:
	1.	State the limitation upfront:
	â€¢	â€œSynthetic dataset contains very few high-risk events; patient-level deterioration classification targets were largely zero. Standard classification tasks were infeasible.â€
	2.	Pivot your narrative to learnable outcomes:
	â€¢	LightGBM: Predict patient-level NEWS2 / continuous risk burden, analyze feature importances to show clinical insights.
	â€¢	TCN: Predict timestamp-level NEWS2 trends to capture dynamic risk evolution.
	3.	Metrics and comparison:
	â€¢	Report regression metrics (RMSE, RÂ², MAE).
	â€¢	Compare to simple baselines (mean NEWS2, last observation carried forward) to show your model improves predictive performance.
	â€¢	Highlight trend detection and feature influence, which is a clinically relevant skill.
	4.	Why this is still strong for a portfolio:
	â€¢	Demonstrates data wrangling, preprocessing, CV, feature engineering, ML pipeline, model selection, hyperparameter tuning, and neural networks.
	â€¢	Shows clinical insight (feature importance, temporal trends).
	â€¢	Recruiters and technical reviewers care about how you solved real-world limitations, not just â€œpredicted rare events.â€